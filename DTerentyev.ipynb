{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMEGsLenA6WI6KiiP8uSMuV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sbnetaa/stepik_dataengineer_pyspark_testtask/blob/main/DTerentyev.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQzNzlvSXGSJ",
        "outputId": "bf1d4ead-34e2-4cb0-e83b-91d45316a30f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 active IP addresses:\n",
            "+---------------+-------------+\n",
            "|             ip|request_count|\n",
            "+---------------+-------------+\n",
            "|123.255.100.159|            1|\n",
            "|197.182.145.148|            1|\n",
            "|   171.60.45.31|            1|\n",
            "| 198.198.74.105|            1|\n",
            "|113.158.126.157|            1|\n",
            "|   45.21.87.120|            1|\n",
            "|   111.90.67.21|            1|\n",
            "|  4.200.165.157|            1|\n",
            "| 178.84.229.200|            1|\n",
            "| 211.250.205.47|            1|\n",
            "+---------------+-------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "Request count by HTTP method:\n",
            "+------+------------+\n",
            "|method|method_count|\n",
            "+------+------------+\n",
            "|  POST|       24906|\n",
            "|DELETE|       25166|\n",
            "|   PUT|       24906|\n",
            "|   GET|       25022|\n",
            "+------+------------+\n",
            "\n",
            "Number of 404 response codes: 24853\n",
            "Total response size by day:\n",
            "+----------+-------------------+\n",
            "|      date|total_response_size|\n",
            "+----------+-------------------+\n",
            "|2025-01-01|            5362640|\n",
            "|2025-01-02|            5246872|\n",
            "|2025-01-03|            5351104|\n",
            "|2025-01-04|            5018738|\n",
            "|2025-01-05|            5623980|\n",
            "|2025-01-06|            5395643|\n",
            "|2025-01-07|            5155977|\n",
            "|2025-01-08|            5323756|\n",
            "|2025-01-09|            5580046|\n",
            "|2025-01-10|            5142382|\n",
            "|2025-01-11|            5639860|\n",
            "|2025-01-12|            5172720|\n",
            "|2025-01-13|            5006249|\n",
            "|2025-01-14|            5423700|\n",
            "|2025-01-15|            5215123|\n",
            "|2025-01-16|            4984347|\n",
            "|2025-01-17|            5408975|\n",
            "|2025-01-18|            5418529|\n",
            "|2025-01-19|            5484012|\n",
            "|2025-01-20|            5189904|\n",
            "+----------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Dmitriy T. test task\").master(\"local[*]\").getOrCreate()\n",
        "\n",
        "column_names = [\"ip\", \"timestamp\", \"method\", \"url\", \"response_code\", \"response_size\"]\n",
        "\n",
        "dataframe = spark.createDataFrame(schema=column_names, data=spark.read.csv(path=\"/content/web_server_logs.csv\", header=True).collect())\n",
        "\n",
        "logs_temp_view = dataframe.createOrReplaceTempView(\"logs\")\n",
        "\n",
        "\n",
        "\n",
        "# 1. Сгруппируйте данные по IP и посчитайте количество запросов для каждого IP, выводим 10 самых активных IP. Формат вывода, как на скрине ниже.\n",
        "print(\"Top 10 active IP addresses:\")\n",
        "spark.sql(\"SELECT ip, COUNT(method) AS request_count FROM logs GROUP BY ip ORDER BY request_count\").show(10)\n",
        "\n",
        "\n",
        "\n",
        "# 2. Сгруппируйте данные по HTTP-методу и посчитайте количество запросов для каждого метода.\n",
        "print(\"Request count by HTTP method:\")\n",
        "spark.sql(\"SELECT method, COUNT(method) AS method_count FROM logs GROUP BY method\").show()\n",
        "\n",
        "\n",
        "\n",
        "# 3. Профильтруйте и посчитайте количество запросов с кодом ответа 404.\n",
        "dataframe.selectExpr(\"cast(COUNT(*) as int) count_all\")\n",
        "count404 = spark.sql(\"SELECT COUNT(*) AS count_all FROM logs WHERE response_code = 404\").take(1)[0].count_all\n",
        "print(\"Number of 404 response codes:\", count404)\n",
        "\n",
        "\n",
        "\n",
        "# 4. Сгруппируйте данные по дате и просуммируйте размер ответов, сортируйте по дате.\n",
        "print(\"Total response size by day:\")\n",
        "spark.sql(\"SELECT DATE_FORMAT(timestamp, 'yyyy-MM-dd') AS date, SUM(CAST(ROUND(response_size) AS INT)) AS total_response_size FROM logs GROUP BY date ORDER BY date\").show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}